{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "LanguageDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxkppMHAHifN"
      },
      "source": [
        "# **Importing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te4BdZi5rkJr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6NXjHaVHybo"
      },
      "source": [
        "# **Input dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "peB_tQcirkKv",
        "outputId": "6d89977c-d98f-4590-e7cc-7c838e66b6b2"
      },
      "source": [
        "df = pd.read_csv(\"/content/dataset.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe4 in position 163: invalid continuation byte",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-336fd65a132d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe4 in position 163: invalid continuation byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz1k_nQprkKy"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GbB5th6rkK0"
      },
      "source": [
        "#Converting all text to lowercase\n",
        "df[\"Text\"] = df[\"Text\"].str.lower()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIluhG_prkK2"
      },
      "source": [
        "max_words = 50000\n",
        "max_len = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL-TYk6OIqwo"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqklntQmrkK4"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = max_words)\n",
        "tokenizer.fit_on_texts(list(df['Text']))\n",
        "train_df = tokenizer.texts_to_sequences(list(df['Text']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoXmGPYYrkK6"
      },
      "source": [
        "train_df = tf.keras.preprocessing.sequence.pad_sequences(train_df,maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMM8nmpZrkK_"
      },
      "source": [
        "len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXKzKorArkLD"
      },
      "source": [
        "Y = df['language']\n",
        "Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwaGbfWIJCZX"
      },
      "source": [
        "# **Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRKLlHB_rkLF"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "le = preprocessing.LabelEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prPZA1B_rkLI"
      },
      "source": [
        "le.fit(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJLVAhVZrkLJ"
      },
      "source": [
        "list(le.classes_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBGB__OcrkLM"
      },
      "source": [
        "Y2 = le.fit_transform(Y)\n",
        "total_languages = df['language'].nunique()\n",
        "Y2 = keras.utils.to_categorical(Y2,num_classes=total_languages)\n",
        "np.shape(Y2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFgSs_wuJXT3"
      },
      "source": [
        "# **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQIUJtANrkLR"
      },
      "source": [
        "X_train,X_test,Y_train,Y_test = train_test_split(train_df,Y2)\n",
        "embedding_dims = 500\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = embedding_dims,input_length = max_len),\n",
        "                            tf.keras.layers.Flatten(),\n",
        "                            tf.keras.layers.Dense(total_languages,activation=tf.nn.softmax)\n",
        "                            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbL5n9UHrkLT"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxzjDxYJrkLU"
      },
      "source": [
        "model.compile(optimizer ='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6J8JLeDrkLV"
      },
      "source": [
        "model.fit(np.array(X_train),np.array(Y_train),epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ks7sqXmrkLX"
      },
      "source": [
        "model.evaluate(np.array(X_test),np.array(Y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Positive Test Cases\n",
        "\n",
        "#MultiLang : Swedish and Dutch\n",
        "#text = [\"När du väl känner till alla element är det inte svårt att ta ihop en mening. Als je eenmaal alle elementen kent, is het niet moeilijk om een zin samen te stellen. \"]\n",
        "\n",
        "\n",
        "#Multilang : Hindi and Arabic\n",
        "text = [\" بمجرد أن تعرف كل العناصر ، ليس من الصعب تجميع جملة معًاएक बार जब आप सभी तत्वों को जान लेते हैं, तो एक वाक्य को एक साथ खींचना मुश्किल नहीं है।\"]\n",
        "\n",
        "#Negative Test Case\n",
        "text=[\"Une incorporation de mots est une classe d'approches permettant de représenter des mots et des documents à l'aide d'une représentation vectorielle dense.Il s'agit d'une amélioration par rapport aux schémas de codage de modèle de sac de mots traditionnels où de grands vecteurs épars étaient utilisés pour représenter chaque mot ou pour marquer chaque mot dans un vecteur pour représenter un vocabulaire entier. Ces représentations étaient rares car les vocabulaires étaient vastes et un mot ou un document donné serait représenté par un grand vecteur composé principalement de valeurs nulles.Au lieu de cela, dans une incorporation, les mots sont représentés par des vecteurs denses où un vecteur représente la projection du mot dans un espace vectoriel continu.La position d'un mot dans l'espace vectoriel est apprise à partir du texte et est basée sur les mots qui entourent le mot lorsqu'il est utilisé.La position d'un mot dans l'espace vectoriel appris est appelée son incorporation.Voici deux exemples populaires de méthodes d'apprentissage de l'intégration deएक बार जब आप सभी तत्वों को जान एक शब्द एम्बेडिंग घने वेक्टर प्रतिनिधित्व का उपयोग करते हुए शब्दों और दस्तावेजों का प्रतिनिधित्व करने के लिए दृष्टिकोणों का एक वर्ग है।यह अधिक पारंपरिक बैग-ऑफ-वर्ड मॉडल एन्कोडिंग योजनाओं पर एक सुधार है जहां बड़े विरल वैक्टर का उपयोग प्रत्येक शब्द का प्रतिनिधित्व करने के लिए या वेक्टर के भीतर प्रत्येक शब्द को पूरी शब्दावली का प्रतिनिधित्व करने के लिए किया जाता था। ये प्रतिनिधित्व विरल थे क्योंकि शब्दसंग्रह विशाल थे और किसी दिए गए शब्द या दस्तावेज को एक बड़े वेक्टर द्वारा दर्शाया जाएगा जिसमें ज्यादातर शून्य मान शामिल थे।इसके बजाय, एक एम्बेडिंग में, शब्दों को घने वैक्टर द्वारा दर्शाया जाता है जहां एक वेक्टर एक निरंतर वेक्टर अंतरिक्ष में शब्द के प्रक्षेपण का प्रतिनिधित्व करता है।\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaxKOqLSEUmO"
      },
      "source": [
        "## **TEST CASES**\n",
        "# Inline single language \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_RdZYP2rkLY"
      },
      "source": [
        "#ENGLISH\n",
        "text = [\" creation of a word essential for encoding A byproduct of words  in our Une incorporation de mots est une classe d'approches permettant de représenter\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfV6mMVLrkLZ"
      },
      "source": [
        "test_text = tokenizer.texts_to_sequences(text)\n",
        "test_text = tf.keras.preprocessing.sequence.pad_sequences(test_text, maxlen=max_len)\n",
        "predictions = model.predict(test_text)\n",
        "out = predictions.argmax()\n",
        "print(le.inverse_transform([out]))\n",
        "print(predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk7baqdSV2P1"
      },
      "source": [
        "# **Dual Language Identification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMLJpSBmEmvj"
      },
      "source": [
        "\n",
        "#Multilang : French\n",
        "text = [\" Une incorporation de mots est une classe d'approches permettant de représenter\"]\n",
        "\n",
        "test_text = tokenizer.texts_to_sequences(text)\n",
        "test_text = tf.keras.preprocessing.sequence.pad_sequences(test_text, maxlen=max_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf-VpyFnk_uB"
      },
      "source": [
        "#------------------Identify two languages by finding the two highest values---------------------#\n",
        "#firstlang\n",
        "predictions = model.predict(test_text)\n",
        "out = predictions.argmax()\n",
        "print(out)\n",
        "print(le.inverse_transform([out]))\n",
        "#converting it to a list\n",
        "a = predictions[0]\n",
        "arr = list()\n",
        "for i in a:\n",
        "  arr.append(i)\n",
        "#secondlang\n",
        "arr[out]=0\n",
        "out2=arr.index(max(arr))\n",
        "print(out2)\n",
        "print(le.inverse_transform([out2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV89Ik4eXU91"
      },
      "source": [
        "# **Multi-Language Detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtH3YMqMTm01"
      },
      "source": [
        "!pip install langdetect\n",
        "from langdetect import detect\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwDswk8lSZAU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu_DVGdyTm06"
      },
      "source": [
        "all_languages_codes =   {\n",
        "  \n",
        "  \t'ar': 'Arabic',\n",
        "  \t'ara': 'Arabic',\n",
        "  \t'zh': 'Chinese',\n",
        "  \t'zh-cn': 'Chinese (Simplified)',\n",
        "  \t'zh-tw': 'Chinese (Traditional)',\n",
        "  \t'chi/zho*': 'Chinese',\n",
        "  \t'nl': 'Dutch',\n",
        "  \t'dut/nld*': 'Dutch',\n",
        "  \t'en': 'English',\n",
        "  \t'eng': 'English',\n",
        "    'et': 'Estonian',\n",
        "  \t'est': 'Estonian',\n",
        "  \t'fr': 'French',\n",
        "  \t'fre/fra*': 'French',\n",
        "   \t'de': 'German',\n",
        "  \t'ger/deu*': 'German',\n",
        "  \t'hi': 'Hindi',\n",
        "  \t'hin': 'Hindi',\n",
        "  \t'ja': 'Japanese',\n",
        "  \t'jpn': 'Japanese',\n",
        "  \t'ko': 'Korean',\n",
        "  \t'kor': 'Korean',\n",
        "  \t'la': 'Latin',\n",
        "  \t'lat': 'Latin',\n",
        "  \t'fa': 'Persian',\n",
        "  \t'per/fas*': 'Persian',\n",
        "  \t'pt': 'Portuguese',\n",
        "  \t'por': 'Portuguese',\n",
        "  \t'ps': 'Pushto',\n",
        "  \t'pus': 'Pushto',\n",
        "  \t'ro': 'Romanian',\n",
        "  \t'rum/ron*': 'Romanian',\n",
        "  \t'ru': 'Russian',\n",
        "  \t'rus': 'Russian',\n",
        "  \t'es': 'Spanish; Castilian',\n",
        "  \t'spa': 'Spanish; Castilian',\n",
        "   \t'sv': 'Swedish',\n",
        "  \t'swe': 'Swedish',\n",
        "  \t'ta': 'Tamil',\n",
        "  \t'tam': 'Tamil',\n",
        "  \t'th': 'Thai',\n",
        "  \t'tha': 'Thai',\n",
        "  \t'tr': 'Turkish',\n",
        "  \t'tur': 'Turkish',\n",
        "   \t'ur': 'Urdu',\n",
        "  \t'urd': 'Urdu',\n",
        "  \t\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQrPCS3tUH8E"
      },
      "source": [
        "testsite_array = []\n",
        "with open('/input .txt') as my_file:\n",
        "    for line in my_file:\n",
        "        testsite_array.append(line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ85TYemVXjB"
      },
      "source": [
        "Column_names = [\"Language Code\", \"Input Sentence\"]\n",
        "format_text = '{:24}' * (len(Column_names) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM-t1nmllviv"
      },
      "source": [
        "print ('\\n', format_text.format(\"Language Name\", *Column_names),'\\n','='*120)\n",
        "for data in testsite_array:\n",
        "    language_code = detect(data)    \n",
        "    sentence = [all_languages_codes.get(language_code), language_code, data]\n",
        "    print (format_text.format(*sentence))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJydvSnHZ8F7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsRTbYAxaucA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdTrIZkUauzU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KU92Ttjau_J"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiWyBd94avCt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TByc6VBdavZr"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeNctVkdave7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFKU2johavi4"
      },
      "source": [
        ""
      ]
    }
  ]
}